# ğŸš² Seoul Bike Sharing Demand Prediction

Advanced Regression Modeling using Pseudo-Labeling & OOF Optimization

## ğŸ“Œ Project Overview

ì´ í”„ë¡œì íŠ¸ëŠ” ì„œìš¸ì‹œ ê³µê³µìì „ê±°(ë”°ë¦‰ì´) ëŒ€ì—¬ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€(Regression) ë¶„ì„ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
ê¸°ìƒ ë°ì´í„°(ê¸°ì˜¨, ìŠµë„, ê°•ìˆ˜ëŸ‰ ë“±)ì™€ ì‹œê³„ì—´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëŒ€ì—¬ëŸ‰ì„ ì˜ˆì¸¡í•˜ë©°, ì´ˆê¸° ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸(RMSE 170+)ì—ì„œ ì‹œì‘í•˜ì—¬ ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ê³¼ ì•™ìƒë¸” ê¸°ë²•, ì¤€ì§€ë„ í•™ìŠµì„ í†µí•´ ìµœì¢… RMSE 105ë¼ëŠ” Top-tier ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ê¸°ê³„í•™ìŠµ Kaggle Leaderbordì—ì„œ 1ë“±í•˜ì˜€ìŠµë‹ˆë‹¤.

## ğŸ† Key Achievements

| Version | Strategy | RMSE Score | Improvement |
| :---: | :--- | :---: | :---: |
| **Baseline** | Linear Regression | 170+ | - |
| **v17** | LGBM Single Model + Feature Selection | 126 | â–² 44 |
| **v30** | 4-Model Ensemble + Rolling Mean Features | 117 | â–² 9 |
| **v42 (Final)** | **Pseudo-Labeling + OOF Optimization** | **105** | **â–² 12** |


## ğŸ’¡ Core Strategies (ë¬¸ì œ í•´ê²° ë…¸í•˜ìš°)

1. Advanced Feature Engineering (ì´ë™ í‰ê·  ë„ì…)

ë‹¨ìˆœí•œ ì‹œì°¨(Lag) í”¼ì²˜ê°€ ë°ì´í„° ëˆ„ìˆ˜(Data Leakage)ì™€ ì˜¤ì—¼ì„ ìœ ë°œí•˜ì—¬ RMSE 700ì ëŒ€ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œì¼°ë˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì´ë™ í‰ê· (Rolling Mean)ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.

shift(1): 1ì‹œê°„ ì „ ë‚ ì”¨

rolling(3).mean(): ìµœê·¼ 3ì‹œê°„ í‰ê·  (ë‹¨ê¸° ì¶”ì„¸)

rolling(24).mean(): ìµœê·¼ 24ì‹œê°„ í‰ê·  (ì¼ì¼ ì¶”ì„¸)

Safety Lock: ë°ì´í„°ê°€ ì—°ì†ë˜ì§€ ì•Šì€ êµ¬ê°„(00ì‹œ, ëˆ„ë½ ë°ì´í„°)ì€ NaN ì²˜ë¦¬í•˜ì—¬ í•™ìŠµ ì˜¤ì—¼ì„ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.

2. Dynamic Feature Selection (ë™ì  íŠ¹ì§• ì„ íƒ)

ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´, XGBoostë¡œ í”¼ì²˜ ì¤‘ìš”ë„(Feature Importance)ë¥¼ ê³„ì‚°í•œ ë’¤ ê¸°ì—¬ë„ê°€ ê°€ì¥ ë‚®ì€ í•˜ìœ„ 3ê°œì˜ í”¼ì²˜ë¥¼ ìë™ìœ¼ë¡œ ì œê±°í•˜ëŠ” ë™ì  ì„ íƒ ë°©ì‹ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.

3. OOF (Out-Of-Fold) Weight Optimization

ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”ì´ ì•„ë‹Œ, êµì°¨ ê²€ì¦ ê³¼ì •ì—ì„œ ë„ì¶œëœ OOF ì˜ˆì¸¡ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ RMSEë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.

Discovery: ì‹¤í—˜ ê²°ê³¼ CatBoostê°€ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

Optimized Weights: CatBoost (73%) + LightGBM (17%) + XGBoost (10%)

4. Pseudo-Labeling (ì˜ì‚¬ ë¼ë²¨ë§)

110ì ëŒ€ì—ì„œ 100ì ëŒ€ë¡œ ì§„ì…í•œ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤. Teacher-Student êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.

Teacher Step: ìµœì í™”ëœ 1ë‹¨ê³„ ëª¨ë¸ì´ Test Setì„ ì˜ˆì¸¡í•˜ì—¬ 'ê°€ì§œ ì •ë‹µ(Pseudo-Label)' ìƒì„±.

Student Step: Train Set + Pseudo-Labeled Test Setì„ í•©ì³ ë°ì´í„°ì…‹ì„ í™•ì¥í•œ ë’¤ ì¬í•™ìŠµ.

## ğŸ“Š Project Workflow

![Seoul Bike Prediction Workflow](workflow.png)

## ğŸ› ï¸ Environment & Libraries

Language: Python 3.x

Environment: Google Colab

Key Libraries:

pandas, numpy: ë°ì´í„° ì „ì²˜ë¦¬

scikit-learn: KFold, RandomForest

xgboost, lightgbm, catboost: í•µì‹¬ ì˜ˆì¸¡ ëª¨ë¸

scipy: ê°€ì¤‘ì¹˜ ìµœì í™”

## ğŸš€ How to Run

# 1. Install dependencies
pip install xgboost lightgbm catboost pandas numpy scikit-learn

# 2. Prepare Data
Ensure 'train.csv' and 'test.csv' are in the correct directory.

# 3. Run the script
python main.ipynb


ğŸ“‚ File Structure

â”œâ”€â”€ main.ipynb          # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì½”ë“œ
|
â”œâ”€â”€ README.md           # í”„ë¡œì íŠ¸ ì„¤ëª… ë¬¸ì„œ
|
â”œâ”€â”€ train.csv           # í•™ìŠµ ë°ì´í„°
|
â””â”€â”€ test.csv            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
